How many particles labeled "synecho" are in the file provided?
> data <- read.csv(file="seaflow_21min.csv",head=TRUE,sep=",")
summary(data)
[1] 18146

What is the 3rd Quantile of the field fsc_small? (the summary function computes this on your behalf)
summary(data)
[1] 39184

What is the mean of the variable "time" for your training set?
> set.seed(1000)
> test_subscript=sample(nrow(data), nrow(data) / 2)
> data_training=data[test_subscript,]
> data_testing=data[-test_subscript,]
> mean(data_training$time)
[1] 342.8615

In the plot of pe vs. chl_small, the particles labeled ultra should appear to be somewhat "mixed" with two other populations of particles.
Which two populations?
> library('ggplot2')
> ggplot(aes(x=chl_small, y=pe, color=pop), data=data)+geom_jitter()

Pico, Nano

Use print(model) to inspect your tree. Which populations, if any, is your tree incapable of recognizing? (Which populations do not appear on any branch?)
(It's possible, but very unlikely, that an incorrect answer to this question is the result of improbable sampling.)
> library(rpart)
> fol <-formula(pop~fsc_small + fsc_perp + chl_small + pe + chl_big + chl_small)
> model_1<-rpart(fol,method="class",data=data_training)
> print(model_1)
> plot(model_1); text(model_1);

crypto

Most trees will include a node near the root that applies a rule to the pe field,
where particles with a value less than some threshold will descend down one branch,
and particles with a value greater than some threshold will descend down a different branch.

If you look at the plot you created previously, you can verify that the threshold used in the tree is evident visually.

What is the value of the threshold on the pe field learned in your model?

5004

Based on your decision tree, which variables appear to be most important in predicting the class population?

chl_small, pe

How accurate was your decision tree on the test data? Enter a number between 0 and 1.
> test_predict_1 <-predict(model_1,newdata=data_testing)
> pop_test_1=c()
> pop_names=c("crypto","nano","pico","synecho","ultra")
> for (i in 1:nrow(test_predict_1)) {pop_test_1<-c(pop_test_1, pop_names[which.max(test_predict_1[i,])])}
> result_1=as.vector(data_testing$pop)==pop_test_1
> table(result_1)
> table(result_1)
result_1
FALSE  TRUE
 5238 30934
> accuracy_1=sum(result_1)/length(pop_test_1)
> print(accuracy_1)
0.8551919

What was the accuracy of your random forest model on the test data? Enter a number between 0 and 1.
plot(model_2)
> test_predict_1 <-predict(model_2,type='prob',newdata=data_testing)
> pop_test_2=c()
> for (i in 1:nrow(test_predict_1)) {pop_test_2<-c(pop_test_2, pop_names[which.max(test_predict_1[i,])])}
> result_2=as.vector(data_testing$pop)==pop_test_2
> table(result_2)
result_2
FALSE  TRUE
 2851 33321
> accuracy_2=sum(result_2)/length(pop_test_2)
0.9211821

After calling importance(model), you should be able to determine which variables appear to be most important in terms of the gini impurity measure. Which ones are they?
> importance(model_2)
          MeanDecreaseGini
fsc_small         2712.221
fsc_perp          1850.638
chl_small         8397.516
pe                9012.867
chl_big           4862.587


What was the accuracy of your support vector machine model on the test data? Enter a number between 0 and 1.
> library(e1071)
> model_3 <- svm(fol, data=data_training)
> test_predict_3=predict(model_3,newdata=data_testing)
> table(pred = test_predict_3, true =data_testing$pop)

Construct a confusion matrix for each of the three methods using the table function. What appears to be the most common error the models make?
> table(pred = test_predict_1, true = data_testing$pop) # Decision tree
> table(pred = test_predict_2, true = data_testing $pop) # Random Forest
> table(pred = test_predict_3, true = data_testing $pop) # Support Vector Machine

After removing data associated with file_id 208, what was the effect on the accuracy of your svm model?
Enter a positive or negative number representing the net change in accuracy,
where a positive number represents an improvement in accuracy and a negative number represents a decrease in accuracy.

plot(data$chl_big, data $chl_small)
plot(data $fsc_big, data $fsc_small)
plot(data $fsc_perp, data $pe)

The variables in the dataset were assumed to be continuous, but one of them takes on only a few discrete values, suggesting a problem. Which variable exhibits this problem?

data_2 = subset(data, data$file_id != 208)

splits = splitdf(data_2, seed=1234)
new_training = splits$data_training
new_test = splits$data_testing

new_svm_model = svm(fol, data=new_training)
new_svm_predict = predict(new_svm_model, newdata=new_test)
new_svm_result = new_svm_predict == new_test$pop
summary(new_svm_result)
